{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss9WR_zbQELO"
   },
   "source": [
    "# Практическое задание 3\n",
    "\n",
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB3vM2GfQELQ"
   },
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете решать задачу извлечения именованных сущностей (Named Entity Recognition) - одну из самых распространенных в NLP наряду с задачей текстовой классификации.\n",
    "\n",
    "Данная задача заключается в том, что нужно классифицировать каждое слово / токен на предмет того, является ли оно частью именованной сущности (сущность может состоять из нескольких слов / токенов) или нет.\n",
    "\n",
    "Например, мы хотим извлечь имена и названия организаций. Тогда для текста\n",
    "\n",
    "    Yan    Goodfellow  works  for  Google  Brain\n",
    "\n",
    "модель должна извлечь следующую последовательность:\n",
    "\n",
    "    B-PER  I-PER       O      O    B-ORG   I-ORG\n",
    "\n",
    "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система (*BIO*-разметка) введена, чтобы различать последовательные именованные сущности одного типа.\n",
    "Существуют и другие типы разметок, например *BILUO*, но в рамках данного практического задания сфокусируемся имеено на *BIO*.\n",
    "\n",
    "Решать NER задачу мы будем на датасете CoNLL-2003 с использованием рекуррентных сетей и моделей на базе архитектуры Transformer.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Основные библиотеки:\n",
    " - [PyTorch](https://pytorch.org/)\n",
    " - [Transformers](https://github.com/huggingface/transformers)\n",
    " \n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве, который состоит из:\n",
    "\n",
    "- *train.tsv* - обучающая выборка. В каждой строке записаны: <слово / токен>, <тэг слова / токена>\n",
    "\n",
    "- *valid.tsv* - валидационная выборка, которую можно использовать для подбора гиперпарамеров и замеров качества. Имеет идентичную с train.tsv структуру.\n",
    "\n",
    "- *test.tsv* - тестовая выборка, по которой оценивается итоговое качество. Имеет идентичную с train.tsv структуру.\n",
    "\n",
    "Скачать данные можно здесь: [ссылка](https://github.com/dayyass/msu_task_3_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S5BCB1EfQan1"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.21.6 scikit-learn==1.0.2 tensorboard==2.9.0 torch==1.12.1 tqdm==4.64.0 transformers==4.21.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Thidpb9qQELS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiDlmbY2QELT"
   },
   "source": [
    "Зафиксируем seed для воспроизводимости результатов (желательно делать **всегда**!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yt3ISg3aQELU"
   },
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_global_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhIg0ZBzQELV"
   },
   "source": [
    "Проинициализируем device (CPU / GPU) на котором будем работать (желательно **GPU**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rboLOv95QELV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW16ryFIQELW"
   },
   "source": [
    "Здесь и далее проинициализируем *tensorboard* для логгирования метрики в процессе обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6O7Y8hReTODp"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs\n",
    "# ЗАПУСКАЮ ТЕНЗОРБОРД В ОТДЕЛЬНОЙ ВКЛАДКЕ, НЕ ТУТ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k3Nhd3IQELY"
   },
   "source": [
    "## Часть 1. Подготовка данных (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qYjOMuPQELY"
   },
   "source": [
    "Первым делом нам нужно считать данные. Давайте напишем функцию, которая на вход принимает путь до одного из conll-2003 файла и возвращает два списка:\n",
    "- список списков слов / токенов (и соответствующий ему)\n",
    "- список списков тегов\n",
    "\n",
    "P.S. Сделаем данную функцию более гибкой, подавая на вход еще булеву переменную, считываем ли мы данные в *lowercase* или нет.\n",
    "\n",
    "**Задание. Реализуйте функцию read_conll2003.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wQdCfX2OQELZ"
   },
   "outputs": [],
   "source": [
    "def read_conll2003(\n",
    "    path: str,\n",
    "    lower: bool = True,\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Prepare data in CoNNL like format.\n",
    "    \"\"\"\n",
    "\n",
    "    token_seq = []\n",
    "    label_seq = []\n",
    "\n",
    "    local_token_seq = []\n",
    "    local_label_seq = []\n",
    "    with open(path, \"r\") as f:\n",
    "        while curline := f.readline():\n",
    "            if curline != \"\\n\":\n",
    "                token, label = curline.split()\n",
    "                if lower:\n",
    "                    token = token.lower()\n",
    "                local_token_seq.append(token)\n",
    "                local_label_seq.append(label)\n",
    "            else:\n",
    "                token_seq.append(local_token_seq)\n",
    "                label_seq.append(local_label_seq)\n",
    "                local_token_seq = []\n",
    "                local_label_seq = []\n",
    "\n",
    "    return token_seq, label_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYm8xEvFQELb"
   },
   "source": [
    "Считаем все три файла:\n",
    "- *train.tsv*\n",
    "- *valid.tsv*\n",
    "- *test.tsv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-inr1BPgQELb",
    "outputId": "125203fe-c32e-459d-e659-d1a999712147"
   },
   "outputs": [],
   "source": [
    "train_token_seq, train_label_seq = read_conll2003(\"data/train.tsv\")\n",
    "valid_token_seq, valid_label_seq = read_conll2003(\"data/valid.tsv\")\n",
    "test_token_seq, test_label_seq = read_conll2003(\"data/test.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOoNc1VUQELc"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HK8AcwWGQELd",
    "outputId": "d949fff7-3fb9-4e3e-c2fa-853d449f8314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu\tB-ORG\n",
      "rejects\tO\n",
      "german\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "british\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(train_token_seq[0], train_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8SqDeMJjF3Y",
    "outputId": "4939b43d-4fe6-4125-a383-6cb89b573d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket\tO\n",
      "-\tO\n",
      "leicestershire\tB-ORG\n",
      "take\tO\n",
      "over\tO\n",
      "at\tO\n",
      "top\tO\n",
      "after\tO\n",
      "innings\tO\n",
      "victory\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(valid_token_seq[0], valid_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddFE7p5kjF_p",
    "outputId": "b000a8f1-49b9-4022-905e-78077ee2e666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\tO\n",
      "-\tO\n",
      "japan\tB-LOC\n",
      "get\tO\n",
      "lucky\tO\n",
      "win\tO\n",
      ",\tO\n",
      "china\tB-PER\n",
      "in\tO\n",
      "surprise\tO\n",
      "defeat\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(test_token_seq[0], test_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZ4Go3IXfDit",
    "outputId": "f5d3dc81-6aa2-4311-8168-b0c9dc0976d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(train_token_seq) == len(\n",
    "    train_label_seq\n",
    "), \"Длины тренировочных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "assert len(valid_token_seq) == len(\n",
    "    valid_label_seq\n",
    "), \"Длины валидационных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "assert len(test_token_seq) == len(\n",
    "    test_label_seq\n",
    "), \"Длины тестовых token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "\n",
    "assert train_token_seq[0] == [\n",
    "    \"eu\",\n",
    "    \"rejects\",\n",
    "    \"german\",\n",
    "    \"call\",\n",
    "    \"to\",\n",
    "    \"boycott\",\n",
    "    \"british\",\n",
    "    \"lamb\",\n",
    "    \".\",\n",
    "], \"Ошибка в тренировочном token_seq\"\n",
    "assert train_label_seq[0] == [\n",
    "    \"B-ORG\",\n",
    "    \"O\",\n",
    "    \"B-MISC\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"B-MISC\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "], \"Ошибка в тренировочном label_seq\"\n",
    "\n",
    "assert valid_token_seq[0] == [\n",
    "    \"cricket\",\n",
    "    \"-\",\n",
    "    \"leicestershire\",\n",
    "    \"take\",\n",
    "    \"over\",\n",
    "    \"at\",\n",
    "    \"top\",\n",
    "    \"after\",\n",
    "    \"innings\",\n",
    "    \"victory\",\n",
    "    \".\",\n",
    "], \"Ошибка в валидационном token_seq\"\n",
    "assert valid_label_seq[0] == [\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"B-ORG\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "], \"Ошибка в валидационном label_seq\"\n",
    "\n",
    "assert test_token_seq[0] == [\n",
    "    \"soccer\",\n",
    "    \"-\",\n",
    "    \"japan\",\n",
    "    \"get\",\n",
    "    \"lucky\",\n",
    "    \"win\",\n",
    "    \",\",\n",
    "    \"china\",\n",
    "    \"in\",\n",
    "    \"surprise\",\n",
    "    \"defeat\",\n",
    "    \".\",\n",
    "], \"Ошибка в тестовом token_seq\"\n",
    "assert test_label_seq[0] == [\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"B-LOC\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"B-PER\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "], \"Ошибка в тестовом label_seq\"\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j96zKo6PQELd"
   },
   "source": [
    "Датасет CoNLL-2003 представлен в виде разметки **BIO**, где лейбл:\n",
    "- *B-{label}* - начало сущности *{label}*\n",
    "- *I-{label}* - продолжение сущности *{label}*\n",
    "- *O* - отсутсвие сущности\n",
    "\n",
    "Также существует другие разметки последовательностей, например **BILUO**. Подробнее с разметками можно ознакомится во вспомогательном ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SVL4USbQELe"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "Чтобы обучать нейронную сеть, мы будем использовать два отображения:\n",
    "- {**token**}→{**token_idx**}: соответствие между словом / токеном и строкой в *embedding* матрице (начинается с 0);\n",
    "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
    "\n",
    "Теперь нам необходимо реализовать две функции:\n",
    "- get_token2idx\n",
    "- get_label2idx\n",
    "\n",
    "которые будут возвращать соответствующие словари.\n",
    "\n",
    "P.S. token2idx словарь должен также содержать специальные токены:\n",
    "- `<PAD>` - спецтокен для паддинга, так как мы собираемся обучать модели батчами\n",
    "- `<UNK>` - спецтокен для обработки слов / токенов, которых нет в словаре (актуально для инференса)\n",
    "\n",
    "Давайте для удобства дадим им idx 0 и 1 соответственно.\n",
    "\n",
    "P.P.S. В get_token2idx можно также добавить параметр *min_count*, который будет включать только слова превышающие определенную частоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOnc3UHpQELf"
   },
   "source": [
    "Сначала соберем:\n",
    "- token2cnt - словарь из уникального слова / токена в количество это слова / токена в тренировочной выборке (важно, что только в тренировочной!)\n",
    "- label_set - список из уникальных тегов\n",
    "\n",
    "P.S. Также можно использовать стемминг для того, чтобы преобразовывать разные словоформы одного слова в один токен, но мы опустим этот момент.\n",
    "\n",
    "**Задание. Реализуйте функции get_token2idx и get_label2idx.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IthnXKsoo7A3"
   },
   "outputs": [],
   "source": [
    "token2cnt = Counter([token for sentence in train_token_seq for token in sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_v8YUM7QELg",
    "outputId": "77f22cb2-a640-43e5-996b-fcaa2020ce99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 8390),\n",
       " ('.', 7374),\n",
       " (',', 7290),\n",
       " ('of', 3815),\n",
       " ('in', 3621),\n",
       " ('to', 3424),\n",
       " ('a', 3199),\n",
       " ('and', 2872),\n",
       " ('(', 2861),\n",
       " (')', 2861)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2cnt.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSm7B546nmDh",
    "outputId": "d6863a06-52a2-42da-859f-41097e761673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов в тренировочном датасете: 21010\n",
      "Количество слов встречающихся только один раз в тренировочном датасете: 10060\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество уникальных слов в тренировочном датасете: {len(token2cnt)}\")\n",
    "print(\n",
    "    f\"Количество слов встречающихся только один раз в тренировочном датасете: {len([token for token, cnt in token2cnt.items() if cnt == 1])}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRtCHt1QruSU"
   },
   "source": [
    "Как мы видим, у нас есть много слов, которые встречаются только один раз в датасете. Очевидно, что выучиться по ним у нас не получиться, мы только переобучимся, поэтому давайте выкинем такие слова при формировании нашего словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aCaPftCyQELi"
   },
   "outputs": [],
   "source": [
    "# используйте параметр min_count для того, чтобы отсекать слова частотой cnt < min_count\n",
    "\n",
    "\n",
    "def get_token2idx(\n",
    "    token2cnt: dict[str, int],\n",
    "    min_count: int,\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    token2idx: dict[str, int] = {}\n",
    "\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "\n",
    "    token_list = [token for token, cnt in token2cnt.items() if cnt >= min_count]\n",
    "    for num, token in tqdm(enumerate(token_list), total=len(token_list)):\n",
    "        token2idx[token] = num + 2\n",
    "\n",
    "    return token2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFK130y-sLH4",
    "outputId": "35f031b5-9388-48c4-8f8a-e77b5a3e580f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cb92eb38ee4cea8228f782f62466fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token2idx = get_token2idx(token2cnt, min_count=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "g69HFZC7QELh"
   },
   "outputs": [],
   "source": [
    "# Функция для сортировки тегов, чтобы сначала был тег O, потом теги B- и только после теги I- (можно задать вручную)\n",
    "\n",
    "\n",
    "def sort_labels_func(x: str) -> int:\n",
    "    if x == \"O\":\n",
    "        return 0\n",
    "    elif x.startswith(\"B-\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "label_set = sorted(\n",
    "    set(label for sentence in train_label_seq for label in sentence),\n",
    "    key=lambda x: (sort_labels_func(x), x),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VI_3m4qbQELi",
    "outputId": "116bfcab-c56b-4ad9-b0f8-894ead418318"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t6i51GPtQELj"
   },
   "outputs": [],
   "source": [
    "def get_label2idx(label_set: list[str]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from labels to indices.\n",
    "    \"\"\"\n",
    "\n",
    "    label2idx: dict[str, int] = {}\n",
    "\n",
    "    for cnt, label in tqdm(enumerate(label_set), total=len(label_set)):\n",
    "        label2idx[label] = cnt\n",
    "\n",
    "    return label2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XW6fK0HtQELk",
    "outputId": "17d18b48-55fa-4603-b342-e61409199577"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1775019c00ac408190e1468e9e03252a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label2idx = get_label2idx(label_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U13l-2IOQELk"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7U7bMrHQELl",
    "outputId": "53d80294-e80b-4536-acfa-99bf171d2e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\t0\n",
      "<UNK>\t1\n",
      "eu\t2\n",
      "german\t3\n",
      "call\t4\n",
      "to\t5\n",
      "boycott\t6\n",
      "british\t7\n",
      "lamb\t8\n",
      ".\t9\n"
     ]
    }
   ],
   "source": [
    "for token, idx in list(token2idx.items())[:10]:\n",
    "    print(f\"{token}\\t{idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hp75V-o2QELl",
    "outputId": "7f85dca1-eea6-4789-e261-1552bcaf91d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t0\n",
      "B-LOC\t1\n",
      "B-MISC\t2\n",
      "B-ORG\t3\n",
      "B-PER\t4\n",
      "I-LOC\t5\n",
      "I-MISC\t6\n",
      "I-ORG\t7\n",
      "I-PER\t8\n"
     ]
    }
   ],
   "source": [
    "for label, idx in label2idx.items():\n",
    "    print(f\"{label}\\t{idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYb4BdAUhNzk",
    "outputId": "dc62e8d8-7b62-4638-c436-aaf657dc5287"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e424d651d0004b69af6709a4b7782640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    len(get_token2idx(token2cnt, min_count=1)) == 21012\n",
    "), \"Ошибка в длине словаря, скорее всего неверно реализован min_count\"\n",
    "assert (\n",
    "    len(token2idx) == 10952\n",
    "), \"Неправильная длина token2idx, скорее всего неверно реализован min_count\"\n",
    "assert len(label2idx) == 9, \"Неправильная длина label2idx\"\n",
    "\n",
    "assert list(token2idx.items())[:10] == [\n",
    "    (\"<PAD>\", 0),\n",
    "    (\"<UNK>\", 1),\n",
    "    (\"eu\", 2),\n",
    "    (\"german\", 3),\n",
    "    (\"call\", 4),\n",
    "    (\"to\", 5),\n",
    "    (\"boycott\", 6),\n",
    "    (\"british\", 7),\n",
    "    (\"lamb\", 8),\n",
    "    (\".\", 9),\n",
    "], \"Неправильно сформированный token2idx\"\n",
    "assert label2idx == {\n",
    "    \"O\": 0,\n",
    "    \"B-LOC\": 1,\n",
    "    \"B-MISC\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"B-PER\": 4,\n",
    "    \"I-LOC\": 5,\n",
    "    \"I-MISC\": 6,\n",
    "    \"I-ORG\": 7,\n",
    "    \"I-PER\": 8,\n",
    "}, \"Неправильно сформированный label2idx\"\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItPs1DmOQELm"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины.\n",
    "\n",
    "Из предыдущего практического задания вы должны знать о `Dataset`'е (`torch.utils.data.Dataset`) - структура данных, которая хранит и может по индексу отдавать данные для обучения. Датасет должен наследоваться от стандартного PyTorch класса Dataset и переопределять методы `__len__` и `__getitem__`.\n",
    "\n",
    "Метод `__getitem__` должен возвращать индексированную последовательность и её теги.\n",
    "\n",
    "**Не забудьте** про `<UNK>` спецтокен для неизвестных слов!\n",
    "    \n",
    "Давайте напишем кастомный датасет под нашу задачу, который на вход (метод `__init__`) будет принимать:\n",
    "- token_seq - список списков слов / токенов\n",
    "- label_seq - список списков тегов\n",
    "- token2idx\n",
    "- label2idx\n",
    "\n",
    "и возвращать из метода `__getitem__` два int64 тензора (`torch.LongTensor`) из индексов слов / токенов в сэмпле и индексов соответвующих тегов:\n",
    "\n",
    "**Задание. Реализуйте класс датасета NERDataset.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kdZvnUUpQELm"
   },
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: list[list[str]],\n",
    "        label_seq: list[list[str]],\n",
    "        token2idx: dict[str, int],\n",
    "        label2idx: dict[str, int],\n",
    "    ):\n",
    "        self.token2idx = token2idx\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "        self.token_seq = [\n",
    "            self.process_tokens(tokens, token2idx) for tokens in token_seq\n",
    "        ]\n",
    "        self.label_seq = [\n",
    "            self.process_labels(labels, label2idx) for labels in label_seq\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        return (\n",
    "            torch.LongTensor(self.token_seq[idx]),\n",
    "            torch.LongTensor(self.label_seq[idx]),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def process_tokens(\n",
    "        tokens: list[str],\n",
    "        token2idx: dict[str, int],\n",
    "        unk: str = \"<UNK>\",\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Transform list of tokens into list of tokens' indices.\n",
    "        \"\"\"\n",
    "        unk_token = token2idx[unk]\n",
    "        return list([token2idx.get(token, unk_token) for token in tokens])\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: list[str],\n",
    "        label2idx: dict[str, int],\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "        \"\"\"\n",
    "        return list([label2idx[label] for label in labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCvaPJERQELn"
   },
   "source": [
    "Создадим три датасета:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bUMsSNkoQELn"
   },
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIq1pAWQELo"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_Scync0QELo",
    "outputId": "1669dbf8-2b23-40e0-9012-430cd44c9bc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 3, 4, 5, 6, 7, 8, 9]), tensor([3, 0, 2, 0, 0, 0, 2, 0, 0]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyAazaLzjQ-K",
    "outputId": "ed864972-9051-4b85-e206-30e59d9971c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1737,  571, 1777,  197,  687,  145,  349,  111, 1819, 1558,    9]),\n",
       " tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHuuh3YmjRNt",
    "outputId": "5f214ff3-7bbb-4b7c-8c9c-a861562b09cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9]),\n",
       " tensor([0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gox6uyF2idwZ",
    "outputId": "8abb0480-9913-4f8a-d50e-1713611cbc9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
    "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
    "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
    "\n",
    "assert torch.equal(\n",
    "    train_dataset[0][0], torch.tensor([2, 1, 3, 4, 5, 6, 7, 8, 9])\n",
    "), \"Неправильно сформированный train_dataset\"\n",
    "assert torch.equal(\n",
    "    train_dataset[0][1], torch.tensor([3, 0, 2, 0, 0, 0, 2, 0, 0])\n",
    "), \"Неправильно сформированный train_dataset\"\n",
    "\n",
    "assert torch.equal(\n",
    "    valid_dataset[0][0],\n",
    "    torch.tensor([1737, 571, 1777, 197, 687, 145, 349, 111, 1819, 1558, 9]),\n",
    "), \"Неправильно сформированный valid_dataset\"\n",
    "assert torch.equal(\n",
    "    valid_dataset[0][1], torch.tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "), \"Неправильно сформированный valid_dataset\"\n",
    "\n",
    "assert torch.equal(\n",
    "    test_dataset[0][0],\n",
    "    torch.tensor([1516, 571, 1434, 1729, 4893, 2014, 67, 310, 215, 3157, 3139, 9]),\n",
    "), \"Неправильно сформированный test_dataset\"\n",
    "assert torch.equal(\n",
    "    test_dataset[0][1], torch.tensor([0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0])\n",
    "), \"Неправильно сформированный test_dataset\"\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjJuAk7QELp"
   },
   "source": [
    "Для того, чтобы дополнять последовательности паддингом, будем использовать параметр `collate_fn` класса `DataLoader`.\n",
    "\n",
    "Принимая последовательность пар тензоров для предложений и тегов, необходимо дополнить все последовательности до последовательности максимальной длины в батче.\n",
    "\n",
    "Используйте для дополнения спецтокен `<PAD>` для последовательностей слов / токенов и -1 для последовательностей тегов.\n",
    "\n",
    "**hint**: удобно использовать метод **torch.nn.utils.rnn**. Обратите особое внимание на параметр *batch_first*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiJVM5qQELp"
   },
   "source": [
    "`Collator` можно реализовать двумя способами:\n",
    "- класс с методом `__call__`\n",
    "- функцию\n",
    "\n",
    "Мы пойдем первым путем.\n",
    "\n",
    "Инициализировать экземпляр класса `Collator` (метод `__init__`) с помощью двух параметров:\n",
    "- id `<PAD>` спецтокена для последовательностей слов / токенов\n",
    "- id `<PAD>` спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов\n",
    "    \n",
    "P.S. `<PAD>` значение нужно для того, чтобы при подсчете лосса легко отличать западдированные токены от других. Можно использовать параметр *ignore_index* при инициализации лосса.\n",
    "\n",
    "**Задание. Реализуйте класс коллатора NERCollator.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LNHNwoLnQELp"
   },
   "outputs": [],
   "source": [
    "class NERCollator:\n",
    "    \"\"\"\n",
    "    Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_padding_value: int,\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: list[tuple[torch.LongTensor, torch.LongTensor]],\n",
    "    ) -> tuple[torch.LongTensor, torch.LongTensor]:\n",
    "\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            tokens, batch_first=True, padding_value=self.token_padding_value\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=self.label_padding_value\n",
    "        )\n",
    "\n",
    "        return tokens, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nZUMwVQTQELq"
   },
   "outputs": [],
   "source": [
    "collator = NERCollator(\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    label_padding_value=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsgfij8WQELq"
   },
   "source": [
    "Теперь всё готово, чтобы задать `DataLoader`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gFljkiBOQELr"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False,  # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False,  # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i34wGJ4uQELr"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QLlr_DztQELr"
   },
   "outputs": [],
   "source": [
    "tokens, labels = next(iter(train_dataloader))\n",
    "\n",
    "tokens = tokens.to(device)\n",
    "labels = labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdMMEDdbQELs",
    "outputId": "b7bd1bb5-248b-482c-cd5e-c98c6c3709f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7796, 1162, 2553, 7237, 1342,    0,    0,    0,    0,    0],\n",
       "        [ 125, 1167,    1,   67, 1349,  489, 1215, 1364, 1365, 1366]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w--fhADKQELs",
    "outputId": "708d1cec-7e0c-4f07-f723-900aab50203c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  0,  3,  7,  0, -1, -1, -1, -1, -1],\n",
       "        [ 0,  4,  8,  0,  1,  0,  0,  0,  0,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFeX0AYKlhGk",
    "outputId": "3c5a0193-2caf-4d7c-f02f-272b91e0f6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels = next(\n",
    "    iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert torch.equal(\n",
    "    train_tokens,\n",
    "    torch.tensor([[2, 1, 3, 4, 5, 6, 7, 8, 9], [10, 11, 0, 0, 0, 0, 0, 0, 0]]),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    train_labels,\n",
    "    torch.tensor([[3, 0, 2, 0, 0, 0, 2, 0, 0], [4, 8, -1, -1, -1, -1, -1, -1, -1]]),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "valid_tokens, valid_labels = next(\n",
    "    iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert torch.equal(\n",
    "    valid_tokens,\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1737, 571, 1777, 197, 687, 145, 349, 111, 1819, 1558, 9],\n",
    "            [248, 10679, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    valid_labels,\n",
    "    torch.tensor(\n",
    "        [[0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "test_tokens, test_labels = next(\n",
    "    iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert torch.equal(\n",
    "    test_tokens,\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1516, 571, 1434, 1729, 4893, 2014, 67, 310, 215, 3157, 3139, 9],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    test_labels,\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0],\n",
    "            [4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ul5gLriQELs"
   },
   "source": [
    "## Часть 2. BiLSTM-теггер (6 баллов)\n",
    "\n",
    "Определите архитектуру сети, используя библиотеку PyTorch. \n",
    "\n",
    "Ваша архитектура в этом пункте должна соответствовать стандартному теггеру:\n",
    "* Embedding слой на входе\n",
    "* LSTM (однонаправленный или двунаправленный)слой для обработки последовательности\n",
    "* Dropout (заданный отдельно или встроенный в LSTM) для уменьшения переобучения\n",
    "* Linear слой на выходе\n",
    "\n",
    "Для обучения сети используйте поэлементную кросс-энтропийную функцию потерь.\n",
    "\n",
    "**Обратите внимание**, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь. В качестве оптимизатора рекомендуется использовать Adam. Для получения значений предсказаний по выходам модели используйте функцию `argmax`.\n",
    "\n",
    "**Задание. Реализуйте класс модели BiLSTM.** **<font color='red'>(2 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMiLQljZQELt"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        bidirectional: bool,\n",
    "        n_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.head = torch.nn.Linear(\n",
    "            (2 if bidirectional else 1) * hidden_size, n_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
    "        embed = self.embedding(tokens)\n",
    "\n",
    "        # используем специальную функцию pack_padded_sequence для того, чтобы получить структуру PackedSequence\n",
    "        # которая не учитывать паддинг при проходе rnn\n",
    "        length = (tokens != 0).sum(dim=1).detach().cpu()\n",
    "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embed, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # используем специальную функцию pad_packed_sequence для того, чтобы получить тензор из PackedSequence\n",
    "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
    "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_rnn_output, batch_first=True\n",
    "        )\n",
    "\n",
    "        logits = self.head(rnn_output)\n",
    "        return logits.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zps8HL2VQELu"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=100,\n",
    "    hidden_size=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmg2_C_oQELu",
    "outputId": "cc700776-3bbe-43a3-d0db-049ad74965e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(10952, 100)\n",
       "  (rnn): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (head): Linear(in_features=200, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDvWB5J2QELv"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn5Pu1UKQELv"
   },
   "outputs": [],
   "source": [
    "outputs = model(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n02Bsh8eQELw",
    "outputId": "6b704a7d-2cc9-49b0-a260-d4f6f52d2405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert outputs.shape == torch.Size([2, 9, 10])\n",
    "assert 2 < criterion(outputs, labels) < 3\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjhkK9QFQELu"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше 0.76. \n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4hdrFZ9iRPi"
   },
   "outputs": [],
   "source": [
    "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/BiLSTMModel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruMTBSkQQELx"
   },
   "source": [
    "**Задание. Реализуйте функцию подсчета метрик compute_metrics.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkpo3JgWQELx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    outputs: torch.Tensor,\n",
    "    labels: torch.LongTensor,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute NER metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    y_pred = outputs.argmax(1)[labels != -1].cpu()\n",
    "    y_true = labels[labels != -1].cpu()\n",
    "\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "    )\n",
    "\n",
    "    # precision\n",
    "    precision_micro = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"micro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    precision_macro = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    precision_weighted = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # recall\n",
    "    recall_micro = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"micro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    recall_macro = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    recall_weighted = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # f1\n",
    "    f1_micro = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"micro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    f1_macro = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    f1_weighted = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    metrics[\"accuracy\"] = accuracy\n",
    "\n",
    "    metrics[\"precision_micro\"] = precision_micro\n",
    "    metrics[\"precision_macro\"] = precision_macro\n",
    "    metrics[\"precision_weighted\"] = precision_weighted\n",
    "\n",
    "    metrics[\"recall_micro\"] = recall_micro\n",
    "    metrics[\"recall_macro\"] = recall_macro\n",
    "    metrics[\"recall_weighted\"] = recall_weighted\n",
    "\n",
    "    metrics[\"f1_micro\"] = f1_micro\n",
    "    metrics[\"f1_macro\"] = f1_macro\n",
    "    metrics[\"f1_weighted\"] = f1_weighted\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzj89UygQEL0"
   },
   "source": [
    "**Задание. Реализуйте функции обучения и тестирования train_epoch и evaluate_epoch.** **<font color='red'>(2 балла)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG3vQbc_QEL0"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One training cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    for i, (tokens, labels) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "        def closure():\n",
    "            outputs = model(tokens)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            # Подсчет лосса и шаг оптимизатора\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(\n",
    "                \"batch loss / train\", loss.item(), epoch * len(dataloader) + i\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure=closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(tokens)\n",
    "            model.train()\n",
    "\n",
    "        batch_metrics = compute_metrics(\n",
    "            outputs=outputs_inference,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "            writer.add_scalar(\n",
    "                f\"batch {metric_name} / train\",\n",
    "                metric_value,\n",
    "                epoch * len(dataloader) + i,\n",
    "            )\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "    writer.add_scalar(\"loss / train\", avg_loss, epoch)\n",
    "\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "        writer.add_scalar(f\"{metric_name} / train\", metric_value, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4ztFogtQEL0"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One evaluation cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=\"loop over test batches\",\n",
    "        ):\n",
    "\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(tokens)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Подсчет лосса\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(\n",
    "                \"batch loss / test\", loss.item(), epoch * len(dataloader) + i\n",
    "            )\n",
    "\n",
    "            batch_metrics = compute_metrics(\n",
    "                outputs=outputs,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "                writer.add_scalar(\n",
    "                    f\"batch {metric_name} / test\",\n",
    "                    metric_value,\n",
    "                    epoch * len(dataloader) + i,\n",
    "                )\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "        writer.add_scalar(\"loss / test\", avg_loss, epoch)\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
    "            writer.add_scalar(f\"{metric_name} / test\", np.mean(metric_value), epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7Z5MTNzQEL1"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        evaluate_epoch(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTxfU0BfQEL1"
   },
   "source": [
    "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz6mjGZUQEL2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4e118859034245942efbb408065b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.713151037895123\n",
      "\n",
      "Train accuracy: 0.8173141027266931\n",
      "\n",
      "Train precision_micro: 0.8173141027266931\n",
      "\n",
      "Train precision_macro: 0.3592585652464284\n",
      "\n",
      "Train precision_weighted: 0.710792766972431\n",
      "\n",
      "Train recall_micro: 0.8173141027266931\n",
      "\n",
      "Train recall_macro: 0.3913753592258223\n",
      "\n",
      "Train recall_weighted: 0.8173141027266931\n",
      "\n",
      "Train f1_micro: 0.8173141027266931\n",
      "\n",
      "Train f1_macro: 0.36775471367013507\n",
      "\n",
      "Train f1_weighted: 0.7526487861506055\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62393fc0775471cb2e1bea4cdc05cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.5533478270909745\n",
      "\n",
      "Test accuracy: 0.8430531229243555\n",
      "\n",
      "Test precision_micro: 0.8430531229243555\n",
      "\n",
      "Test precision_macro: 0.5770533991568113\n",
      "\n",
      "Test precision_weighted: 0.7764009537830144\n",
      "\n",
      "Test recall_micro: 0.8430531229243555\n",
      "\n",
      "Test recall_macro: 0.6088866653803825\n",
      "\n",
      "Test recall_weighted: 0.8430531229243555\n",
      "\n",
      "Test f1_micro: 0.8430531229243555\n",
      "\n",
      "Test f1_macro: 0.585314610336197\n",
      "\n",
      "Test f1_weighted: 0.8004229230102066\n",
      "\n",
      "Epoch [2 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f56195bcc5a42339bec6415553969d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4405462764042885\n",
      "\n",
      "Train accuracy: 0.8765153665263364\n",
      "\n",
      "Train precision_micro: 0.8765153665263364\n",
      "\n",
      "Train precision_macro: 0.5426243022773235\n",
      "\n",
      "Train precision_weighted: 0.8187797732768449\n",
      "\n",
      "Train recall_micro: 0.8765153665263364\n",
      "\n",
      "Train recall_macro: 0.5436436313406119\n",
      "\n",
      "Train recall_weighted: 0.8765153665263364\n",
      "\n",
      "Train f1_micro: 0.8765153665263364\n",
      "\n",
      "Train f1_macro: 0.5322649910652713\n",
      "\n",
      "Train f1_weighted: 0.8395559040291989\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987d1f0fb9b64c4c96f751594507631b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.39851055053406437\n",
      "\n",
      "Test accuracy: 0.8863599630633631\n",
      "\n",
      "Test precision_micro: 0.8863599630633631\n",
      "\n",
      "Test precision_macro: 0.6759308814190744\n",
      "\n",
      "Test precision_weighted: 0.8452761756530419\n",
      "\n",
      "Test recall_micro: 0.8863599630633631\n",
      "\n",
      "Test recall_macro: 0.6870600851308348\n",
      "\n",
      "Test recall_weighted: 0.8863599630633631\n",
      "\n",
      "Test f1_micro: 0.8863599630633631\n",
      "\n",
      "Test f1_macro: 0.6742405519498011\n",
      "\n",
      "Test f1_weighted: 0.8588359659439405\n",
      "\n",
      "Epoch [3 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16082a70a56f4bdfb24bf7a5ed34dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.32665539107294594\n",
      "\n",
      "Train accuracy: 0.90708184903555\n",
      "\n",
      "Train precision_micro: 0.90708184903555\n",
      "\n",
      "Train precision_macro: 0.6516140991719178\n",
      "\n",
      "Train precision_weighted: 0.8747097781778415\n",
      "\n",
      "Train recall_micro: 0.90708184903555\n",
      "\n",
      "Train recall_macro: 0.6420515971400896\n",
      "\n",
      "Train recall_weighted: 0.90708184903555\n",
      "\n",
      "Train f1_micro: 0.90708184903555\n",
      "\n",
      "Train f1_macro: 0.6354076170784531\n",
      "\n",
      "Train f1_weighted: 0.8842114404200899\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55a75b6d2d14f52a42ee708b2b405ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.3313733645599937\n",
      "\n",
      "Test accuracy: 0.9053369687319254\n",
      "\n",
      "Test precision_micro: 0.9053369687319254\n",
      "\n",
      "Test precision_macro: 0.7318380326327333\n",
      "\n",
      "Test precision_weighted: 0.8770558780676053\n",
      "\n",
      "Test recall_micro: 0.9053369687319254\n",
      "\n",
      "Test recall_macro: 0.7362510937078794\n",
      "\n",
      "Test recall_weighted: 0.9053369687319254\n",
      "\n",
      "Test f1_micro: 0.9053369687319254\n",
      "\n",
      "Test f1_macro: 0.7269577815005684\n",
      "\n",
      "Test f1_weighted: 0.8852633994692095\n",
      "\n",
      "Epoch [4 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3576ea15912743e882f6caa952aac3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2580579658738353\n",
      "\n",
      "Train accuracy: 0.9267781731265516\n",
      "\n",
      "Train precision_micro: 0.9267781731265516\n",
      "\n",
      "Train precision_macro: 0.7188385265228201\n",
      "\n",
      "Train precision_weighted: 0.9072820333582133\n",
      "\n",
      "Train recall_micro: 0.9267781731265516\n",
      "\n",
      "Train recall_macro: 0.7054171916042782\n",
      "\n",
      "Train recall_weighted: 0.9267781731265516\n",
      "\n",
      "Train f1_micro: 0.9267781731265516\n",
      "\n",
      "Train f1_macro: 0.7017713247102332\n",
      "\n",
      "Train f1_weighted: 0.911670787318596\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f641e3ddd01f4897808d75f88c1aec5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.2903325768153\n",
      "\n",
      "Test accuracy: 0.9160764939336647\n",
      "\n",
      "Test precision_micro: 0.9160764939336647\n",
      "\n",
      "Test precision_macro: 0.7620582479967722\n",
      "\n",
      "Test precision_weighted: 0.8994073729319695\n",
      "\n",
      "Test recall_micro: 0.9160764939336647\n",
      "\n",
      "Test recall_macro: 0.7626796279840556\n",
      "\n",
      "Test recall_weighted: 0.9160764939336647\n",
      "\n",
      "Test f1_micro: 0.9160764939336647\n",
      "\n",
      "Test f1_macro: 0.7556017433762103\n",
      "\n",
      "Test f1_weighted: 0.9024698998218964\n",
      "\n",
      "Epoch [5 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5775280923ef4fddb6c38426e6c7d4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20773006342159017\n",
      "\n",
      "Train accuracy: 0.9411402804558606\n",
      "\n",
      "Train precision_micro: 0.9411402804558606\n",
      "\n",
      "Train precision_macro: 0.7690760737170423\n",
      "\n",
      "Train precision_weighted: 0.9286451186137533\n",
      "\n",
      "Train recall_micro: 0.9411402804558606\n",
      "\n",
      "Train recall_macro: 0.7551220538156385\n",
      "\n",
      "Train recall_weighted: 0.9411402804558606\n",
      "\n",
      "Train f1_micro: 0.9411402804558606\n",
      "\n",
      "Train f1_macro: 0.7527934505290259\n",
      "\n",
      "Train f1_weighted: 0.9304885271740045\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6132e83b44b8414f9608a67686057feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.26340041444727125\n",
      "\n",
      "Test accuracy: 0.9231598068579464\n",
      "\n",
      "Test precision_micro: 0.9231598068579464\n",
      "\n",
      "Test precision_macro: 0.7845386281029504\n",
      "\n",
      "Test precision_weighted: 0.9074462025525235\n",
      "\n",
      "Test recall_micro: 0.9231598068579464\n",
      "\n",
      "Test recall_macro: 0.7819966179375342\n",
      "\n",
      "Test recall_weighted: 0.9231598068579464\n",
      "\n",
      "Test f1_micro: 0.9231598068579464\n",
      "\n",
      "Test f1_macro: 0.776904483620156\n",
      "\n",
      "Test f1_weighted: 0.910357289070181\n",
      "\n",
      "Epoch [6 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653820af7394477b9108b7fb02886a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.17586101082016273\n",
      "\n",
      "Train accuracy: 0.9507195751293349\n",
      "\n",
      "Train precision_micro: 0.9507195751293349\n",
      "\n",
      "Train precision_macro: 0.8049252429983307\n",
      "\n",
      "Train precision_weighted: 0.9426366917024647\n",
      "\n",
      "Train recall_micro: 0.9507195751293349\n",
      "\n",
      "Train recall_macro: 0.7907034355347755\n",
      "\n",
      "Train recall_weighted: 0.9507195751293349\n",
      "\n",
      "Train f1_micro: 0.9507195751293349\n",
      "\n",
      "Train f1_macro: 0.789222256773022\n",
      "\n",
      "Train f1_weighted: 0.9429178276674955\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa73707e47204528912c7520b7962c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.24211815479918558\n",
      "\n",
      "Test accuracy: 0.9290993590587537\n",
      "\n",
      "Test precision_micro: 0.9290993590587537\n",
      "\n",
      "Test precision_macro: 0.7987853496805001\n",
      "\n",
      "Test precision_weighted: 0.9171147858879227\n",
      "\n",
      "Test recall_micro: 0.9290993590587537\n",
      "\n",
      "Test recall_macro: 0.7977055083964164\n",
      "\n",
      "Test recall_weighted: 0.9290993590587537\n",
      "\n",
      "Test f1_micro: 0.9290993590587537\n",
      "\n",
      "Test f1_macro: 0.7921406491035488\n",
      "\n",
      "Test f1_weighted: 0.9185884862487578\n",
      "\n",
      "Epoch [7 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e9e7d4dc8e4586be45fba4c2760e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1464321587522387\n",
      "\n",
      "Train accuracy: 0.9597815120437571\n",
      "\n",
      "Train precision_micro: 0.9597815120437571\n",
      "\n",
      "Train precision_macro: 0.8348536590295191\n",
      "\n",
      "Train precision_weighted: 0.9536030126522409\n",
      "\n",
      "Train recall_micro: 0.9597815120437571\n",
      "\n",
      "Train recall_macro: 0.8231298925699155\n",
      "\n",
      "Train recall_weighted: 0.9597815120437571\n",
      "\n",
      "Train f1_micro: 0.9597815120437571\n",
      "\n",
      "Train f1_macro: 0.8215517347838576\n",
      "\n",
      "Train f1_weighted: 0.9536101803791536\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e8b4a17c8348b98894494401e30176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.23163475611267742\n",
      "\n",
      "Test accuracy: 0.9324052059524841\n",
      "\n",
      "Test precision_micro: 0.9324052059524841\n",
      "\n",
      "Test precision_macro: 0.8057062083821729\n",
      "\n",
      "Test precision_weighted: 0.9288393633831574\n",
      "\n",
      "Test recall_micro: 0.9324052059524841\n",
      "\n",
      "Test recall_macro: 0.8048664980117786\n",
      "\n",
      "Test recall_weighted: 0.9324052059524841\n",
      "\n",
      "Test f1_micro: 0.9324052059524841\n",
      "\n",
      "Test f1_macro: 0.7994485471884393\n",
      "\n",
      "Test f1_weighted: 0.9266298160873664\n",
      "\n",
      "Epoch [8 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e3e0224f7543cd8e72d45d2b8b5dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.12246200388687631\n",
      "\n",
      "Train accuracy: 0.9667441327999046\n",
      "\n",
      "Train precision_micro: 0.9667441327999046\n",
      "\n",
      "Train precision_macro: 0.8617145076207257\n",
      "\n",
      "Train precision_weighted: 0.9627878601731062\n",
      "\n",
      "Train recall_micro: 0.9667441327999046\n",
      "\n",
      "Train recall_macro: 0.8507469787842697\n",
      "\n",
      "Train recall_weighted: 0.9667441327999046\n",
      "\n",
      "Train f1_micro: 0.9667441327999046\n",
      "\n",
      "Train f1_macro: 0.8495981806527558\n",
      "\n",
      "Train f1_weighted: 0.9620882601113678\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49e69eea2bf48ecade0486334c8296b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.21930595880956516\n",
      "\n",
      "Test accuracy: 0.937371166501171\n",
      "\n",
      "Test precision_micro: 0.937371166501171\n",
      "\n",
      "Test precision_macro: 0.8200126734008241\n",
      "\n",
      "Test precision_weighted: 0.9283895220903677\n",
      "\n",
      "Test recall_micro: 0.937371166501171\n",
      "\n",
      "Test recall_macro: 0.8166590138598845\n",
      "\n",
      "Test recall_weighted: 0.937371166501171\n",
      "\n",
      "Test f1_micro: 0.937371166501171\n",
      "\n",
      "Test f1_macro: 0.8128242616315339\n",
      "\n",
      "Test f1_weighted: 0.9288666418222451\n",
      "\n",
      "Epoch [9 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e920f4779d146ea848e1e662eaeb0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1045321443313428\n",
      "\n",
      "Train accuracy: 0.9719413376691025\n",
      "\n",
      "Train precision_micro: 0.9719413376691025\n",
      "\n",
      "Train precision_macro: 0.8831669534017594\n",
      "\n",
      "Train precision_weighted: 0.9691401934003814\n",
      "\n",
      "Train recall_micro: 0.9719413376691025\n",
      "\n",
      "Train recall_macro: 0.873751386884969\n",
      "\n",
      "Train recall_weighted: 0.9719413376691025\n",
      "\n",
      "Train f1_micro: 0.9719413376691025\n",
      "\n",
      "Train f1_macro: 0.8724779051239453\n",
      "\n",
      "Train f1_weighted: 0.9681763786348122\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac55537917c745699069b22d6d83b0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.2096073092136287\n",
      "\n",
      "Test accuracy: 0.9401911485672608\n",
      "\n",
      "Test precision_micro: 0.9401911485672608\n",
      "\n",
      "Test precision_macro: 0.8249927314783027\n",
      "\n",
      "Test precision_weighted: 0.9315401435269445\n",
      "\n",
      "Test recall_micro: 0.9401911485672608\n",
      "\n",
      "Test recall_macro: 0.8228705708536168\n",
      "\n",
      "Test recall_weighted: 0.9401911485672608\n",
      "\n",
      "Test f1_micro: 0.9401911485672608\n",
      "\n",
      "Test f1_macro: 0.8187030828916415\n",
      "\n",
      "Test f1_weighted: 0.9320665790278084\n",
      "\n",
      "Epoch [10 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b6969adacf4b9cbd1593de0fbc78d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08830601029717021\n",
      "\n",
      "Train accuracy: 0.9768269910515512\n",
      "\n",
      "Train precision_micro: 0.9768269910515512\n",
      "\n",
      "Train precision_macro: 0.9000865475729021\n",
      "\n",
      "Train precision_weighted: 0.9746393412063573\n",
      "\n",
      "Train recall_micro: 0.9768269910515512\n",
      "\n",
      "Train recall_macro: 0.8924430407787615\n",
      "\n",
      "Train recall_weighted: 0.9768269910515512\n",
      "\n",
      "Train f1_micro: 0.9768269910515512\n",
      "\n",
      "Train f1_macro: 0.8911354365401346\n",
      "\n",
      "Train f1_weighted: 0.9738311425410123\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146cd4b5d6b846748000e604f6712112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.20833139111627058\n",
      "\n",
      "Test accuracy: 0.9415497443768276\n",
      "\n",
      "Test precision_micro: 0.9415497443768276\n",
      "\n",
      "Test precision_macro: 0.8349088504472075\n",
      "\n",
      "Test precision_weighted: 0.9353356315309661\n",
      "\n",
      "Test recall_micro: 0.9415497443768276\n",
      "\n",
      "Test recall_macro: 0.8311630106891403\n",
      "\n",
      "Test recall_weighted: 0.9415497443768276\n",
      "\n",
      "Test f1_micro: 0.9415497443768276\n",
      "\n",
      "Test f1_macro: 0.8278847066793089\n",
      "\n",
      "Test f1_weighted: 0.9348351266936136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=100,\n",
    "    hidden_size=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "train(\n",
    "    10, model, train_dataloader, valid_dataloader, optimizer, criterion, writer, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы по обучению BiLSTM\n",
    "\n",
    "Оказывается, что если использовать дефолтные параметры - сетка и так работает исправно. Вообще, любое машинное обучение это в первую очередь рандом и во все остальные очереди математика. Поэтому я предлагаю свой метод машинного обучения. Более того, я реализовал его на чистом питоне:\n",
    "```python\n",
    "import random\n",
    "\n",
    "random.randint(0, n)\n",
    "```\n",
    "Где n - количество выходных классов. Далее можно запускать, пока на валидационной выборке не получится адекватного результата. Подзравляю, вы получили красивое чиселко, можете отдать его своему боссу (преподу), все равно ваше мл-решение не будет интегрировано, поскольку интегрировать мл в прод - непрактично. Вы можете не поверить, но предложенный мною метод самый быстрый и из-за этого самый cost-effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8R6nopyQEL-"
   },
   "source": [
    "## Часть 3. Transformers-теггер (6 баллов)\n",
    "\n",
    "В данной части задания нужно сделать все то же самое, но с использованием модели на базе архитектуры Transformer, а именно предлагается дообучать предобученную модель **BERT**.\n",
    "\n",
    "Для данной модели подразумевается специальная подготовка данных, с чего мы и начнем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrbX5gFDQEL-"
   },
   "source": [
    "Модель **BERT** использует специальный токенизатор WordPiece для разбиения предложений на токены. Готовая предобученная версия такого токенизатора существует в библиотеке **transformers**. Есть два класса: `BertTokenizer` и `BertTokenizerFast`. Использовать можно любой, но второй вариант работает существенно быстрее.\n",
    "\n",
    "Токенизаторы можно обучать с нуля на своем корпусе данных, а можно подгружать уже готовые. Готовые токенизаторы, как правило, соответствуют предобученной конфигурации модели, которая использует словарь из этого токенизатора. \n",
    "\n",
    "Мы будем использовать базовую конфигурацию предобученного **BERT** для модели и токенизатора.\n",
    "\n",
    "P.S. Часто приходится проводить эксперименты с моделями разной архитектуры, например **BERT** и **GPT**, поэтому удобно использовать класс `AutoTokenizer`, который по названию модели сам определит, какой класс нужен для инициализации токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-UTiI4gQEL-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSbBhvnDQEMA"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-cased\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxWNX5i6QEMA"
   },
   "source": [
    "Подгружение предобученных моделей и токенизаторов в **huggingface** происходит с помощью конструктора **from_pretrained**.\n",
    "\n",
    "В данном конструкторе можно указать либо путь к предобученному токенизатору, либо название предобученной конфигурации, как в нашем случае: тогда **transformers** сам подгрузит нужные параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tg_bCeaQEMA"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MIrbmNoQEMA"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "В сравнении с рекуррентными моделями, на больше не нужно заниматься сборкой словаря, так как это уже сделано заранее благодаря токенизаторам и алгоритмам, стоящими за ними.\n",
    "\n",
    "Но нам как и прежде потребуется:\n",
    "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
    "\n",
    "Но данное отображение у нас уже реализовано в одной из предыдущих частей задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvYF-4uaQEMB"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Мы также хотим обучать модель батчами, поэтому нам как и прежде понадобятся `Dataset`, `Collator` и `DataLoader`.\n",
    "\n",
    "Но мы не можем переиспользовать те, что в предыдущих частях задания, так как обработка данных должна производится немного иначе с использованием токенизатора.\n",
    "\n",
    "Давайте напишем новый кастомный датасет, который на вход (метод `__init__`) будет принимать:\n",
    "- token_seq - список списков слов / токенов\n",
    "- label_seq - список списков тегов\n",
    "\n",
    "и возвращать из метода `__getitem__` два списка:\n",
    "- список текстовых значений (`List[str]`) из индексов токенов в сэмпле\n",
    "- список целочисленных значений (`List[int]`) из индексов соответвующих тегов\n",
    "\n",
    "P.S. В отличие от предыдущего кастомного датасет, здесь мы возвращаем два `List`'а вместо `torch.LongTensor`, так как логику формирования западдированного батча мы перенесем в `Collator` из-за специфики работы токенизатора - он сам возвращает уже западдированный тензор с индексами токенов, а для индексов тегов нам нужно будет сделать это самостоятельно по аналогии с предыдущим датасетом.\n",
    "\n",
    "**Задание. Реализуйте класс датасета TransformersDataset.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EoNLDOOQEMB"
   },
   "outputs": [],
   "source": [
    "class TransformersDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Transformers Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: list[list[str]],\n",
    "        label_seq: list[list[str]],\n",
    "    ):\n",
    "        self.token_seq = token_seq\n",
    "        self.label_seq = [\n",
    "            self.process_labels(labels, label2idx) for labels in label_seq\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> tuple[list[str], list[int]]:\n",
    "        return list([self.token_seq[idx], self.label_seq[idx]])\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: list[str],\n",
    "        label2idx: dict[str, int],\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "        \"\"\"\n",
    "        return list([label2idx[label] for label in labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1oNc-31QEMB"
   },
   "source": [
    "Создадим три датасета:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqg56Jf8QEMC"
   },
   "outputs": [],
   "source": [
    "train_dataset = TransformersDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    ")\n",
    "valid_dataset = TransformersDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    ")\n",
    "test_dataset = TransformersDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdIS6XrvQEMC"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IT00Pjy6QEMC",
    "outputId": "6bb42a27-08f0-49f5-96f5-52573a1a75af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       " [3, 0, 2, 0, 0, 0, 2, 0, 0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYal2icQmuD-",
    "outputId": "572bab3f-d53d-46d2-dd6f-367042966062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cricket',\n",
       "  '-',\n",
       "  'leicestershire',\n",
       "  'take',\n",
       "  'over',\n",
       "  'at',\n",
       "  'top',\n",
       "  'after',\n",
       "  'innings',\n",
       "  'victory',\n",
       "  '.'],\n",
       " [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCXd3FWVmuKe",
    "outputId": "e35bfbaf-340f-4fba-cc39-fa84ce632cda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['soccer',\n",
       "  '-',\n",
       "  'japan',\n",
       "  'get',\n",
       "  'lucky',\n",
       "  'win',\n",
       "  ',',\n",
       "  'china',\n",
       "  'in',\n",
       "  'surprise',\n",
       "  'defeat',\n",
       "  '.'],\n",
       " [0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4R605vAnYT9",
    "outputId": "1b6542ed-4520-4093-dddf-f9404e059917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
    "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
    "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
    "\n",
    "assert train_dataset[0][0] == [\n",
    "    \"eu\",\n",
    "    \"rejects\",\n",
    "    \"german\",\n",
    "    \"call\",\n",
    "    \"to\",\n",
    "    \"boycott\",\n",
    "    \"british\",\n",
    "    \"lamb\",\n",
    "    \".\",\n",
    "], \"Неправильно сформированный train_dataset\"\n",
    "assert train_dataset[0][1] == [\n",
    "    3,\n",
    "    0,\n",
    "    2,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    2,\n",
    "    0,\n",
    "    0,\n",
    "], \"Неправильно сформированный train_dataset\"\n",
    "\n",
    "assert valid_dataset[0][0] == [\n",
    "    \"cricket\",\n",
    "    \"-\",\n",
    "    \"leicestershire\",\n",
    "    \"take\",\n",
    "    \"over\",\n",
    "    \"at\",\n",
    "    \"top\",\n",
    "    \"after\",\n",
    "    \"innings\",\n",
    "    \"victory\",\n",
    "    \".\",\n",
    "], \"Неправильно сформированный valid_dataset\"\n",
    "assert valid_dataset[0][1] == [\n",
    "    0,\n",
    "    0,\n",
    "    3,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "], \"Неправильно сформированный valid_dataset\"\n",
    "\n",
    "assert test_dataset[0][0] == [\n",
    "    \"soccer\",\n",
    "    \"-\",\n",
    "    \"japan\",\n",
    "    \"get\",\n",
    "    \"lucky\",\n",
    "    \"win\",\n",
    "    \",\",\n",
    "    \"china\",\n",
    "    \"in\",\n",
    "    \"surprise\",\n",
    "    \"defeat\",\n",
    "    \".\",\n",
    "], \"Неправильно сформированный test_dataset\"\n",
    "assert test_dataset[0][1] == [\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    4,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "], \"Неправильно сформированный test_dataset\"\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zP_6iQnQEMC"
   },
   "source": [
    "Реализуем новый `Collator`.\n",
    "\n",
    "Инициализировать коллатор будет 3 аргументами:\n",
    "- токенизатор\n",
    "- параметры токенизатора в виде словаря (затем используем как `**kwargs`)\n",
    "- id спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов\n",
    "\n",
    "**Задание. Реализуйте класс коллатора TransformersCollator.** **<font color='red'>(2 балла)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BonAp65jQEMD"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "class TransformersCollator:\n",
    "    \"\"\"\n",
    "    Transformers Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        tokenizer_kwargs: dict[str, Any],\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs\n",
    "\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: list[tuple[list[str], list[int]]],\n",
    "    ) -> tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        tokens = list(map(list, tokens))\n",
    "        tokens = self.tokenizer(tokens, **self.tokenizer_kwargs)\n",
    "\n",
    "        labels_new = torch.zeros_like(tokens[\"input_ids\"])\n",
    "        for batch in range(len(labels_new)):\n",
    "            last_idx = -1\n",
    "            word_ids = tokens.word_ids(batch)\n",
    "            for i in range(len(word_ids)):\n",
    "                word_idx = word_ids[i]\n",
    "                if not (word_idx is None):\n",
    "                    if last_idx != word_idx:\n",
    "                        last_idx = word_idx\n",
    "                        labels_new[batch][i] = labels[batch][word_idx]\n",
    "                    else:\n",
    "                        labels_new[batch][i] = self.label_padding_value\n",
    "                else:\n",
    "                    labels_new[batch][i] = self.label_padding_value\n",
    "\n",
    "        labels = labels_new\n",
    "\n",
    "        tokens.pop(\"offset_mapping\")\n",
    "\n",
    "        return tokens, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_labels(\n",
    "        tokens: BatchEncoding,\n",
    "        labels: list[list[int]],\n",
    "        label_padding_value: int,\n",
    "    ) -> torch.LongTensor:\n",
    "\n",
    "        encoded_labels = []\n",
    "\n",
    "        for doc_labels, doc_offset in zip(labels, tokens.offset_mapping):\n",
    "\n",
    "            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * label_padding_value\n",
    "            arr_offset = np.array(doc_offset)\n",
    "\n",
    "            doc_enc_labels[\n",
    "                (arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)\n",
    "            ] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "        return torch.LongTensor(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iC8JkUPnQEMD"
   },
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"is_split_into_words\": True,\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"padding\": True,\n",
    "    \"truncation\": True,\n",
    "    \"max_length\": 512,\n",
    "    \"return_tensors\": \"pt\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sCDaxR6QEMD"
   },
   "outputs": [],
   "source": [
    "collator = TransformersCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    "    label_padding_value=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirev0N_QEMD"
   },
   "source": [
    "Теперь всё готово, чтобы задать `DataLoader`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JDrLC6pQEME"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False,  # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False,  # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3zGjEDHQEME"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSWcYEAWQEME"
   },
   "outputs": [],
   "source": [
    "tokens, labels = next(iter(train_dataloader))\n",
    "\n",
    "tokens = tokens.to(device)\n",
    "labels = labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTcdU1BlQEME",
    "outputId": "94ac83e6-a90e-4247-a8c9-46fb487d6d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1103,  1441,  1180,  1339,  4917,  4004,  1146,  1106,  1565,\n",
       "          1106,  1275,  1201,  1107,  3315,   117,  1119,  1163,   119,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1103,  1362,   112,   188,  2026,   173,  4359,  2895,  1163,\n",
       "          1107,  8943,  1115,  1122,  1108,  9591,  2480,  1122,  1180,  2080,\n",
       "          1820,  1554,   118,  1214,  5022,  6386,  1120,  1103,  2166,  1214,\n",
       "           112,   188,  3102,   119,   130,  1550,   176, 19118,  1468,   117,\n",
       "          1133,  1896,  1263,   118,  1858, 19743,  1127,  1363,   119,   102]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7ZTh97-QEME",
    "outputId": "7eb951ed-2bf6-475a-f799-07551fc3b5c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "        [-1,  0,  0,  0, -1,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0, -1,  0, -1, -1,  0,\n",
       "          0, -1, -1,  0,  0,  0,  0, -1, -1,  0,  0,  0,  0, -1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMprtk9bodM9",
    "outputId": "d93f5b97-fbad-4538-f37f-068c7bbac104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels = next(\n",
    "    iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert torch.equal(\n",
    "    train_tokens[\"input_ids\"],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                101,\n",
    "                174,\n",
    "                1358,\n",
    "                22961,\n",
    "                176,\n",
    "                14170,\n",
    "                1840,\n",
    "                1106,\n",
    "                21423,\n",
    "                9304,\n",
    "                10721,\n",
    "                1324,\n",
    "                2495,\n",
    "                12913,\n",
    "                119,\n",
    "                102,\n",
    "            ],\n",
    "            [101, 11109, 1200, 1602, 6715, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    train_tokens[\"attention_mask\"],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    train_labels,\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [-1, 3, -1, 0, 2, -1, 0, 0, 0, 2, -1, -1, 0, -1, 0, -1],\n",
    "            [-1, 4, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "valid_tokens, valid_labels = next(\n",
    "    iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert torch.equal(\n",
    "    valid_tokens[\"input_ids\"],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                101,\n",
    "                5428,\n",
    "                118,\n",
    "                5837,\n",
    "                18117,\n",
    "                5759,\n",
    "                15189,\n",
    "                1321,\n",
    "                1166,\n",
    "                1120,\n",
    "                1499,\n",
    "                1170,\n",
    "                6687,\n",
    "                2681,\n",
    "                119,\n",
    "                102,\n",
    "            ],\n",
    "            [101, 25338, 17996, 1820, 118, 4775, 118, 1476, 102, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    valid_tokens[\"attention_mask\"],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    valid_labels,\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [-1, 0, 0, 3, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
    "            [-1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "test_tokens, test_labels = next(\n",
    "    iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert torch.equal(\n",
    "    test_tokens[\"input_ids\"],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                101,\n",
    "                5862,\n",
    "                118,\n",
    "                179,\n",
    "                26519,\n",
    "                1179,\n",
    "                1243,\n",
    "                6918,\n",
    "                1782,\n",
    "                117,\n",
    "                5144,\n",
    "                1161,\n",
    "                1107,\n",
    "                3774,\n",
    "                3326,\n",
    "                119,\n",
    "                102,\n",
    "            ],\n",
    "            [101, 9468, 3309, 1306, 19122, 2293, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    test_tokens[\"attention_mask\"],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(\n",
    "    test_labels,\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [-1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 4, -1, 0, 0, 0, 0, -1],\n",
    "            [-1, 4, -1, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        ]\n",
    "    ),\n",
    "), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m-taH0SQEMF"
   },
   "source": [
    "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Для задачи NER будем использовать класс `BertForTokenClassification`.\n",
    "\n",
    "По аналогии с токенизаторами, мы можем использовать класс `AutoModelForTokenClassification`, который по названию модели сам определит, какой класс нужен для инициализации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6tq_i7JQEMF"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vma9yj0zQEMF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2idx),\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Imv-6gAQQEMG"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAdHfn4oQEMG"
   },
   "outputs": [],
   "source": [
    "outputs = model(**tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-kTke_8QEMG",
    "outputId": "d3ccee80-a0c4-429a-8d62-787faf8543d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert 2 < criterion(outputs[\"logits\"].transpose(1, 2), labels) < 3\n",
    "\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ana4qGKeHrN"
   },
   "outputs": [],
   "source": [
    "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/Transformer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sNuFPRdQEMH"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше 0.9. \n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IfkN20lrN0J"
   },
   "source": [
    "Вы можете использовать ту же самую функцию train, что и до этого за тем исключением, что вместо инференса `model(tokens)` нужно делать `model(**tokens)`, а вместо `outputs` использовать `outputs[\"logits\"].transpose(1, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iyZUFddzYE5"
   },
   "source": [
    "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iv92wK5P7pjl"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One training cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    for i, (tokens, labels) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "        def closure():\n",
    "            outputs = model(**tokens)[\"logits\"].transpose(1, 2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            # Подсчет лосса и шаг оптимизатора\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(\n",
    "                \"batch loss / train\", loss.item(), epoch * len(dataloader) + i\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure=closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(**tokens)[\"logits\"].transpose(1, 2)\n",
    "            model.train()\n",
    "\n",
    "        batch_metrics = compute_metrics(\n",
    "            outputs=outputs_inference,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "            writer.add_scalar(\n",
    "                f\"batch {metric_name} / train\",\n",
    "                metric_value,\n",
    "                epoch * len(dataloader) + i,\n",
    "            )\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "    writer.add_scalar(\"loss / train\", avg_loss, epoch)\n",
    "\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "        writer.add_scalar(f\"{metric_name} / train\", metric_value, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One evaluation cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=\"loop over test batches\",\n",
    "        ):\n",
    "\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "            # Подсчет лосса\n",
    "            outputs = model(**tokens)[\"logits\"].transpose(1, 2)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(\n",
    "                \"batch loss / test\", loss.item(), epoch * len(dataloader) + i\n",
    "            )\n",
    "\n",
    "            batch_metrics = compute_metrics(\n",
    "                outputs=outputs,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "                writer.add_scalar(\n",
    "                    f\"batch {metric_name} / test\",\n",
    "                    metric_value,\n",
    "                    epoch * len(dataloader) + i,\n",
    "                )\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "        writer.add_scalar(\"loss / test\", avg_loss, epoch)\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
    "            writer.add_scalar(f\"{metric_name} / test\", np.mean(metric_value), epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        evaluate_epoch(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68d805ff9814337b3ef72e11165ddd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1379828500450928\n",
      "\n",
      "Train accuracy: 0.9674708976558934\n",
      "\n",
      "Train precision_micro: 0.9674708976558934\n",
      "\n",
      "Train precision_macro: 0.8608852774526856\n",
      "\n",
      "Train precision_weighted: 0.9634267296294969\n",
      "\n",
      "Train recall_micro: 0.9674708976558934\n",
      "\n",
      "Train recall_macro: 0.8621460589527818\n",
      "\n",
      "Train recall_weighted: 0.9674708976558934\n",
      "\n",
      "Train f1_micro: 0.9674708976558934\n",
      "\n",
      "Train f1_macro: 0.8565334620986793\n",
      "\n",
      "Train f1_weighted: 0.963235041526465\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88ad12ea6354336b8015a2f0dcf89be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.0706300544433066\n",
      "\n",
      "Test accuracy: 0.9797267350688725\n",
      "\n",
      "Test precision_micro: 0.9797267350688725\n",
      "\n",
      "Test precision_macro: 0.9308700508115871\n",
      "\n",
      "Test precision_weighted: 0.9796463470285443\n",
      "\n",
      "Test recall_micro: 0.9797267350688725\n",
      "\n",
      "Test recall_macro: 0.9306310520387604\n",
      "\n",
      "Test recall_weighted: 0.9797267350688725\n",
      "\n",
      "Test f1_micro: 0.9797267350688725\n",
      "\n",
      "Test f1_macro: 0.9285677982301892\n",
      "\n",
      "Test f1_weighted: 0.9782976102726397\n",
      "\n",
      "Epoch [2 / 5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da14d97ffaa34eb589ba92b27482658d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04862208441440676\n",
      "\n",
      "Train accuracy: 0.9904060330924898\n",
      "\n",
      "Train precision_micro: 0.9904060330924898\n",
      "\n",
      "Train precision_macro: 0.9546121374180689\n",
      "\n",
      "Train precision_weighted: 0.9915922272047664\n",
      "\n",
      "Train recall_micro: 0.9904060330924898\n",
      "\n",
      "Train recall_macro: 0.9545554460888203\n",
      "\n",
      "Train recall_weighted: 0.9904060330924898\n",
      "\n",
      "Train f1_micro: 0.9904060330924898\n",
      "\n",
      "Train f1_macro: 0.9524118109004016\n",
      "\n",
      "Train f1_weighted: 0.9902419620677042\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005a261b57a94777937b1a7ed3667854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.06680365837738775\n",
      "\n",
      "Test accuracy: 0.982955789745842\n",
      "\n",
      "Test precision_micro: 0.982955789745842\n",
      "\n",
      "Test precision_macro: 0.9425166762266158\n",
      "\n",
      "Test precision_weighted: 0.9823327653625362\n",
      "\n",
      "Test recall_micro: 0.982955789745842\n",
      "\n",
      "Test recall_macro: 0.942252308480358\n",
      "\n",
      "Test recall_weighted: 0.982955789745842\n",
      "\n",
      "Test f1_micro: 0.982955789745842\n",
      "\n",
      "Test f1_macro: 0.9402743465272182\n",
      "\n",
      "Test f1_weighted: 0.9813883297323875\n",
      "\n",
      "Epoch [3 / 5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4ffc28ef0647b98c742f6bbbe1a33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02600136691812317\n",
      "\n",
      "Train accuracy: 0.9959292894071758\n",
      "\n",
      "Train precision_micro: 0.9959292894071758\n",
      "\n",
      "Train precision_macro: 0.9800324724896357\n",
      "\n",
      "Train precision_weighted: 0.9965016035488632\n",
      "\n",
      "Train recall_micro: 0.9959292894071758\n",
      "\n",
      "Train recall_macro: 0.9803074069161772\n",
      "\n",
      "Train recall_weighted: 0.9959292894071758\n",
      "\n",
      "Train f1_micro: 0.9959292894071758\n",
      "\n",
      "Train f1_macro: 0.9791501000210238\n",
      "\n",
      "Train f1_weighted: 0.9958960637579028\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e27e451ddf64cc683459d3cb2604e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.06463553167093904\n",
      "\n",
      "Test accuracy: 0.9840923260996205\n",
      "\n",
      "Test precision_micro: 0.9840923260996205\n",
      "\n",
      "Test precision_macro: 0.9465057665772183\n",
      "\n",
      "Test precision_weighted: 0.9849128401689274\n",
      "\n",
      "Test recall_micro: 0.9840923260996205\n",
      "\n",
      "Test recall_macro: 0.9456374834466081\n",
      "\n",
      "Test recall_weighted: 0.9840923260996205\n",
      "\n",
      "Test f1_micro: 0.9840923260996205\n",
      "\n",
      "Test f1_macro: 0.9440950607639893\n",
      "\n",
      "Test f1_weighted: 0.9833381362704476\n",
      "\n",
      "Epoch [4 / 5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d593424c0d4145bd8e6ffad0230b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01700604417775702\n",
      "\n",
      "Train accuracy: 0.9975598007561932\n",
      "\n",
      "Train precision_micro: 0.9975598007561932\n",
      "\n",
      "Train precision_macro: 0.9883156346138082\n",
      "\n",
      "Train precision_weighted: 0.9980386270926547\n",
      "\n",
      "Train recall_micro: 0.9975598007561932\n",
      "\n",
      "Train recall_macro: 0.9884307783291869\n",
      "\n",
      "Train recall_weighted: 0.9975598007561932\n",
      "\n",
      "Train f1_micro: 0.9975598007561932\n",
      "\n",
      "Train f1_macro: 0.9878265348708792\n",
      "\n",
      "Train f1_weighted: 0.9976123374205755\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6d2270eef743ecaf8f390ce6099ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.07254851845726364\n",
      "\n",
      "Test accuracy: 0.9840031295592578\n",
      "\n",
      "Test precision_micro: 0.9840031295592578\n",
      "\n",
      "Test precision_macro: 0.9461682910460566\n",
      "\n",
      "Test precision_weighted: 0.9858364549840232\n",
      "\n",
      "Test recall_micro: 0.9840031295592578\n",
      "\n",
      "Test recall_macro: 0.9463470803489635\n",
      "\n",
      "Test recall_weighted: 0.9840031295592578\n",
      "\n",
      "Test f1_micro: 0.9840031295592578\n",
      "\n",
      "Test f1_macro: 0.9442333898689975\n",
      "\n",
      "Test f1_weighted: 0.9837808353390012\n",
      "\n",
      "Epoch [5 / 5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495a3950ace34a309a80822458027f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/7493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011456240796906495\n",
      "\n",
      "Train accuracy: 0.9987805025634398\n",
      "\n",
      "Train precision_micro: 0.9987805025634398\n",
      "\n",
      "Train precision_macro: 0.9940392965705978\n",
      "\n",
      "Train precision_weighted: 0.9990280059837978\n",
      "\n",
      "Train recall_micro: 0.9987805025634398\n",
      "\n",
      "Train recall_macro: 0.9942967542522426\n",
      "\n",
      "Train recall_weighted: 0.9987805025634398\n",
      "\n",
      "Train f1_micro: 0.9987805025634398\n",
      "\n",
      "Train f1_macro: 0.9938536967569437\n",
      "\n",
      "Train f1_weighted: 0.9988090479958545\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa58273762a4f7a9daa41fa5ed0db35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.0739324505152346\n",
      "\n",
      "Test accuracy: 0.9842014279187585\n",
      "\n",
      "Test precision_micro: 0.9842014279187585\n",
      "\n",
      "Test precision_macro: 0.9453683498899998\n",
      "\n",
      "Test precision_weighted: 0.9845634013362247\n",
      "\n",
      "Test recall_micro: 0.9842014279187585\n",
      "\n",
      "Test recall_macro: 0.9449727258497214\n",
      "\n",
      "Test recall_weighted: 0.9842014279187585\n",
      "\n",
      "Test f1_micro: 0.9842014279187585\n",
      "\n",
      "Test f1_macro: 0.9431692569968024\n",
      "\n",
      "Test f1_weighted: 0.9832254410804634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2idx),\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "train(\n",
    "    5, model, train_dataloader, valid_dataloader, optimizer, criterion, writer, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы по обучению трансформеров\n",
    "\n",
    "В целом, выводы те же как и в прошлом разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XlI3cb1QEL2"
   },
   "source": [
    "## Часть 4 - Бонус. BiLSTMAttention-теггер (2 баллa)\n",
    "\n",
    "Необходимо провести те же самые эксперименты как и в части 2, но уже с использованием усовершенствованной архитектуры теггера BiLSTM с Attention механизмом.\n",
    "\n",
    "**Обратите внимание**, что реализовывать Attention самому не нужно, можно использовать `torch.nn.MultiheadAttention`.\n",
    "\n",
    "Также сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров и проведите небольшой сравнительный анализ с предыдущей архитектурой. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook).\n",
    "\n",
    "**Задание. Реализуйте класс модели BiLSTMAttn.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MyLQp047yID"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezh9kLTkQEL9"
   },
   "source": [
    "**Задание. Проведите эксперименты и побейте метрику из части 2.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "P.S. Eсли качества увеличить не получилось, это нужно обосновать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE1C1tzEQEL-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4MIrbmNoQEMA"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.11.0 ('cmc-dl-course-PVluD1ef')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d93dc95b16a871f1477b2d841c91e4ced9d245f1dd127eba776d204cafd6987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
